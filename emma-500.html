<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Massive Language Adaptation of Large Language Models">
    <meta property="og:title" content="MaLA-LM" />
    <meta property="og:description"
        content="MaLA-LM focuses on adapting large language models to support hundreds of languages, including many underrepresented ones." />
    <meta property="og:url" content="mala-lm.github.io" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords"
        content="Large Language Models, Multilingual Models, Continual Pretraining, Instruction Tuning, Evaluation">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>EMMA-500</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">EMMA-500: Enhancing Massively Multilingual Adaptation
                            of Large Language Models</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://shaoxiongji.github.io/" target="_blank">Shaoxiong Ji</a><sup>1*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="http://zihao.cool" target="_blank">Zihao Li</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://ineil77.github.io" target="_blank">Indraneil Paul</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a>Jaakko Paavola</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://lpq29743.github.io/" target="_blank">Peiqin Lin</a><sup>3,5</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://pinzhenchen.github.io/" target="_blank">Pinzhen Chen</a><sup>4</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://dayyanobrien.github.io/" target="_blank">Dayy√°n
                                    O'Brien</a><sup>4</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://researchportal.helsinki.fi/en/persons/hengyu-luo"
                                    target="_blank">Hengyu Luo</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://cisnlp.github.io/" target="_blank">Hinrich Sch√ºtze</a><sup>3,5</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann"
                                    target="_blank">J√∂rg Tiedemann</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://homepages.inf.ed.ac.uk/bhaddow/" target="_blank">Barry
                                    Haddow</a><sup>4</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <sup>1</sup>University of Helsinki
                                <sup>2</sup>Technical University of Darmstadt
                                <sup>3</sup>University of Munich<br>
                                <sup>4</sup>University of Edinburgh
                                <sup>5</sup>Munich Center for Machine Learning<br>

                            </span>
                            <span class="eql-cntrb">
                                <small><br><sup>*</sup>Corresponding to shaoxiong.ji@tu-darmstadt.de</small>
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Arxiv PDF link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2409.17892.pdf" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                                    <a href="https://github.com/MaLA-LM/emma-500/" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2409.17892" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- HuggingFace Link -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/MaLA-LM/emma-500-llama2-7b" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <span role="img" aria-label="Hugging Face Emoji">ü§ó</span>
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>

                                <!-- Discord link -->
                                <span class="link-block">
                                    <a href="https://discord.com/invite/F5mEb7U6we" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-discord"></i>
                                        </span>
                                        <span>Discord</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            In this work, we introduce EMMA-500, a large-scale multilingual language model
                            continue-trained on texts across 546 languages designed for enhanced multilingual
                            performance, focusing on improving language coverage for low-resource languages. To
                            facilitate continual pre-training, we compile the MaLA corpus, a comprehensive multilingual
                            dataset enriched with curated datasets across diverse domains. Leveraging this corpus, we
                            conduct extensive continual pre-training of the Llama 2 7B model, resulting in EMMA-500,
                            which demonstrates robust performance across a wide collection of benchmarks, including a
                            comprehensive set of multilingual tasks. Our results highlight the effectiveness of
                            continual pre-training in expanding large language models' language capacity, particularly
                            for underrepresented languages, demonstrating significant gains in cross-lingual transfer,
                            task generalization, and language adaptability. We release the MaLA corpus, EMMA-500 model
                            weights, scripts, and model generations.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->


    <section class="section">
        <div class="container is-max-desktop content">
            <h2 class="title">MaLA Monolingual Corpus</h2>
            <div class="table-container">
                <table class="table is-bordered is-striped is-narrow is-fullwidth">
                    <caption>Details of the MaLA Monolingual Corpus Versions.</caption>
                    <thead>
                        <tr>
                            <th>Corpus Name</th>
                            <th>Notes</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><a href="https://huggingface.co/datasets/MaLA-LM/mala-monolingual-integration" target="_blank">ü§ó MaLA-LM/mala-monolingual-integration</a></td>
                            <td>Noisy version integrating texts from different sources without cleaning.</td>
                        </tr>
                        <tr>
                            <td><a href="https://huggingface.co/datasets/MaLA-LM/mala-monolingual-filter" target="_blank">ü§ó MaLA-LM/mala-monolingual-filter</a></td>
                            <td>Filtered version with further data filtering.</td>
                        </tr>
                        <tr>
                            <td><a href="https://huggingface.co/datasets/MaLA-LM/mala-monolingual-dedup" target="_blank">ü§ó MaLA-LM/mala-monolingual-dedup</a></td>
                            <td>Deduplicated version removing repeated data points.</td>
                        </tr>
                        <tr>
                            <td><a href="https://huggingface.co/datasets/MaLA-LM/mala-monolingual-split" target="_blank">ü§ó MaLA-LM/mala-monolingual-split</a></td>
                            <td>Final version, split into training and test sets after filtering and deduplication.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <style>
        .table-container {
            overflow-x: auto; /* Enable horizontal scrolling for wide tables */
        }
    </style>


   <section class="section hero">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-full-tablet is-two-thirds-desktop">
                    <h2 class="title is-3">Performance Overview</h2>
                    <figure class="image">
                        <img src="static/images/emma-500-chart.png" alt="Your Image Alt Text"
                            style="max-width: 150%; height: auto;" />
                    </figure>
                    <p>The number of wins, i.e., the number of times EMMA-500 achieves the best or superior performance
                        compared to other models in the same category across various evaluation tasks and benchmarks. We
                        compare our EMMA-500 Llama 2 7B model to decoder-only LLMs of similar parameter size, including
                        (i) 10 Llama 2-based LLMs, (ii) 7 multilingual LLMs and CPT models, and (iii) 8 recent advanced
                        LLMs. If EMMA-500 scores higher than all compared models on a specific benchmark, it is
                        considered a winning case for that particular evaluation. Our EMMA-500 Llama 2 model outperforms
                        most Llama 2-based, multilingual LLMs and CPT models. Remarkably, our model achieves the best
                        performance on Flores200, Glot500-c, and PBC among all the compared baselines. Check out our
                        paper for details!</p>
                </div>
            </div>
        </div>
    </section>


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
@article{ji2024emma,
    title={EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models},
    author={Ji, Shaoxiong and Li, Zihao and Paul, Indraneil and Paavola, Jaakko and Lin, Peiqin and Chen, Pinzhen and O'Brien, Dayy{\'a}n and Luo, Hengyu and Sch{\"u}tze, Hinrich and Tiedemann, J{\"o}rg and others},
    journal={arXiv preprint arXiv:2409.17892},
    year={2024}
}
      </code></pre>
        </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
                            You are free to borrow the source code of this website, we just ask that you link back to
                            this page in the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-HHYRBMV6MN"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-HHYRBMV6MN');
    </script>

</body>

</html>