<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Massive Language Adaptation of Large Language Models">
  <meta property="og:title" content="MaLA-LM" />
  <meta property="og:description"
    content="MaLA-LM focuses on adapting large language models to support hundreds of languages, including many underrepresented ones." />
  <meta property="og:url" content="mala-lm.github.io" />
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="Large Language Models, Multilingual Models, Continual Pretraining, Instruction Tuning, Evaluation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Massive Language Adaptation of Large Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Massive Language Adaptation of Large Language Models</h1>
            <h2 class="subtitle is-3">MaLA-LM</h2>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Discord link -->
                <span class="link-block">
                  <a href="https://discord.com/invite/F5mEb7U6we" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-discord"></i>
                    </span>
                    <span>Discord</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/MaLA-LM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>GitHub</span>
                  </a>
                </span>

                <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/MaLA-LM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <span role="img" aria-label="Hugging Face Emoji">ü§ó</span>
                    </span>
                    <span>HuggingFace</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              Welcome to MaLA-LM (Massive Language Adaptation of Large Language Models)! üåç
            </p>
            <p>
              MaLA-LM focuses on adapting large language models to support hundreds of languages, including many
              underrepresented ones.
              Our models are multilingual, scalable, and optimized for diverse linguistic tasks. We work on data
              construction (e.g., <a
                href="https://huggingface.co/collections/MaLA-LM/mala-corpus-66e05127641a51de34d39529">MaLA corpus</a>
              and <a href="https://huggingface.co/datasets/MaLA-LM/PolyWrite">PolyWrite</a>), continual pretraining
              (e.g., <a href="emma-500.html">EMMA-500</a>, <a href="mala-500.html">MaLA-500</a>, and <a
                href="MixCPT.html">MixCPT</a>), instruction fine-tuning (e.g., <a
                href="https://github.com/hplt-project/monolingual-multilingual-instruction-tuning">mono. vs.
                multilingual Alpaca</a> and <a
                href="https://huggingface.co/collections/MaLA-LM/lucky52-660e5fd24a2ced4b334d63d6">Lucky 52</a>) and
              evaluation (e.g., <a href="https://github.com/MaLA-LM/GlotEval">GlotEval</a>).
            </p>
            <p>
              <strong>Featured üó£Ô∏è</strong> Check out our multilingual LLM collections, featuring models trained to
              handle 500+ languages,
              ideal for global, multilingual applications.
            </p>
            <p>
              Dive into the HuggingFace collections:
              <a href="https://huggingface.co/collections/MaLA-LM/emma-500-66eaa9acf1f512c8915b7166">EMMA-500</a> |
              <a href="https://huggingface.co/collections/MaLA-LM/mala-corpus-66e05127641a51de34d39529">MaLA corpus</a>
              |
              <a href="https://huggingface.co/collections/MaLA-LM/mala-500-660e57f8e53e3cc2ccd31cb9">MaLA-500</a>
            </p>
            <p>
              <strong>Continual Pretraining üìú</strong>
            </p>
            <p>
              <em><a href="emma-500.html">EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language
                  Models</a></em>
            </p>
            <p>
              <em><a href="mala-500.html">MaLA-500: Massive Language Adaptation of Large Language Models</a></em>
            </p>
            <p>
              <em><a href="MixCPT.html">Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs
                  Across Languages and Resources</a></em>
            </p>

            <p>
              <strong>GlotEval üõ†Ô∏è</strong>
            </p>
            <p>
              <em><a href="GlotEval.html">GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language
                  Models</a></em>
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HHYRBMV6MN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-HHYRBMV6MN');
  </script>
</body>

</html>