<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Massive Language Adaptation of Large Language Models">
  <meta property="og:title" content="MaLA-LM" />
  <meta property="og:description"
    content="MaLA-LM focuses on adapting large language models to support hundreds of languages, including many underrepresented ones." />
  <meta property="og:url" content="mala-lm.github.io" />
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="Large Language Models, Multilingual Models, Continual Pretraining, Instruction Tuning, Evaluation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Massive Language Adaptation of Large Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Massive Language Adaptation of Large Language Models</h1>
            <h2 class="subtitle is-3">MaLA-LM</h2>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Discord link -->
                <span class="link-block">
                  <a href="https://discord.com/invite/F5mEb7U6we" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-discord"></i>
                    </span>
                    <span>Discord</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/MaLA-LM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>GitHub</span>
                  </a>
                </span>

                <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/MaLA-LM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <span role="img" aria-label="Hugging Face Emoji">ü§ó</span>
                    </span>
                    <span>HuggingFace</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              Welcome to MaLA-LM (Massive Language Adaptation of Large Language Models)! üåç
            </p>
            <p>
              MaLA-LM focuses on adapting large language models to support hundreds of languages, including many
              underrepresented ones.
              Our models are multilingual, scalable, and optimized for diverse linguistic tasks. We work on data
              construction (e.g., <a
                href="https://huggingface.co/collections/MaLA-LM/mala-corpus-66e05127641a51de34d39529">MaLA corpus</a>
              and <a href="https://huggingface.co/datasets/MaLA-LM/PolyWrite">PolyWrite</a>), continual pretraining
              (e.g., <a href="emma-500.html">EMMA-500</a>, <a href="mala-500.html">MaLA-500</a>, and <a
                href="MixCPT.html">MixCPT</a>), instruction fine-tuning (e.g., <a
                href="https://github.com/hplt-project/monolingual-multilingual-instruction-tuning">mono. vs.
                multilingual Alpaca</a> and <a
                href="https://huggingface.co/collections/MaLA-LM/lucky52-660e5fd24a2ced4b334d63d6">Lucky 52</a>) and
              evaluation (e.g., <a href="https://github.com/MaLA-LM/GlotEval">GlotEval</a>).
            </p>
            <p>
              <strong>Featured üó£Ô∏è</strong> Check out our multilingual LLM collections, featuring models trained to
              handle 500+ languages,
              ideal for global, multilingual applications.
            </p>
            <p>
              Dive into the HuggingFace collections:
              <a href="https://huggingface.co/collections/MaLA-LM/emma-500-66eaa9acf1f512c8915b7166">EMMA-500</a> |
              <a href="https://huggingface.co/collections/MaLA-LM/mala-corpus-66e05127641a51de34d39529">MaLA corpus</a>
              |
              <a href="https://huggingface.co/collections/MaLA-LM/mala-500-660e57f8e53e3cc2ccd31cb9">MaLA-500</a>
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section">
    <div class="container is-max-desktop content">
        <h2 class="title">Latest Updates</h2>
        <ul>
            <li>[2025.06] We release EMMA-500 Llama 3/3.1 models and MaLA bilingual corpus in 2,500+ language pairs üåê<a href="http://mala-lm.github.io/emma-500-gen2.html" target="_blank">EMMA-500 Gen2</a></li>
            <li>[2025.05] We release MaLA OPUS bilingual corpus (2410), aka, parallel corpus, in 16,000+ language pairs ü§ó<a href="https://huggingface.co/datasets/MaLA-LM/mala-opus-dedup-2410" target="_blank">MaLA-LM/mala-opus-dedup-2410</a></li>
            <li>[2025.04] We release a series of CPT models that study the data mixing in continual pre-training ü§ó<a href="https://huggingface.co/collections/Zihao-Li/mixcpt-67ea87fbc68ccdaa83fdc01c" target="_blank">MixCPT</a></li>
            <li>[2025.04] We release the preview of GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models <i class="fab fa-github"></i> <a href="https://github.com/mala-lm/gloteval" target="_blank">GlotEval</a></li>
            <li>[2024.09] We release PolyWrite, a synthetic benchmark for open-ended generation ü§ó<a href="https://huggingface.co/datasets/MaLA-LM/PolyWrite" target="_blank">MaLA-LM/PolyWrite</a></li>
            <li>[2024.09] We release the EMMA-500 Llama 2 model and MaLA monolingual corpus in 939 languages üåê<a href="http://mala-lm.github.io/emma-500.html" target="_blank">EMMA-500</a></li>
            <li>[2024.04] We release Lucky 52 models that study the number of languages for instruction fine-tuning ü§ó<a href="https://huggingface.co/collections/MaLA-LM/lucky52-660e5fd24a2ced4b334d63d6" target="_blank">Lucky 52</a></li>
            <li>[2024.03] We release the MaLA-500 v2 model ü§ó<a href="https://huggingface.co/MaLA-LM/mala-500-10b-v2" target="_blank">MaLA-LM/mala-500-10b-v2</a></li>
            <li>[2024.01] We release the MaLA-500 v1 model based on Llama 2 and LoRA ü§ó<a href="https://huggingface.co/MaLA-LM/mala-500-10b-v1" target="_blank">MaLA-LM/mala-500-10b-v1</a></li>
        </ul>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
        <h2 class="title">Our Works</h2>
        <p>
            <strong>Continual Pretraining üìú</strong>
        </p>
        <p>
            <em><a href="emma-500-gen2.html">Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data</a></em>
        </p>
        <p>
            <em><a href="emma-500.html">EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models</a></em>
        </p>
        <p>
            <em><a href="mala-500.html">MaLA-500: Massive Language Adaptation of Large Language Models</a></em>
        </p>
        <p>
            <em><a href="MixCPT.html">Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources</a></em>
        </p>

        <p>
            <strong>Instruction Fine-tuning üîÆ</strong>
        </p>
        <p>
            <em><a href="https://arxiv.org/abs/2404.04850" target="_blank">How Many Languages Make Good Multilingual Instruction Tuning? A Case Study on BLOOM</a></em>
        </p>
        <p>
            <em><a href="https://arxiv.org/abs/2309.08958" target="_blank">Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca</a></em>
        </p>

        <p>
            <strong>Evaluation üõ†Ô∏è</strong>
        </p>
        <p>
            <em><a href="GlotEval.html">GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models</a></em>
        </p>
    </div>
</section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HHYRBMV6MN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-HHYRBMV6MN');
  </script>
</body>

</html>